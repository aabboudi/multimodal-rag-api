{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "\n",
    "import os\n",
    "import shutil\n",
    "from langchain.document_loaders.pdf import PyPDFDirectoryLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.schema.document import Document\n",
    "from langchain.vectorstores.chroma import Chroma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants\n",
    "\n",
    "CHROMA_PATH = \"./chroma\"\n",
    "DATA_PATH = \"./data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clear existing Chroma database by removing the CHROMA_PATH directory\n",
    "\n",
    "def clear_database():\n",
    "  if os.path.exists(CHROMA_PATH):\n",
    "    shutil.rmtree(CHROMA_PATH)\n",
    "    print(f\"Database at {CHROMA_PATH} has been cleared.\")\n",
    "\n",
    "clear_database()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load PDF documents from the specified data path using the PyPDFDirectoryLoader.\n",
    "\n",
    "document_loader = PyPDFDirectoryLoader(DATA_PATH)\n",
    "documents = document_loader.load()\n",
    "print(f\"Loaded {len(documents)} documents.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split documents into smaller chunks\n",
    "CHUNK_SIZE=800\n",
    "CHUNK_OVERLAP=80\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "  chunk_size=CHUNK_SIZE,\n",
    "  chunk_overlap=CHUNK_OVERLAP,\n",
    "  length_function=len,\n",
    "  is_separator_regex=False\n",
    ")\n",
    "chunks = text_splitter.split_documents(documents)\n",
    "print(f\"Split documents into {len(chunks)} chunks.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculates unique chunk IDs based on the document source and page\n",
    "\n",
    "def calculate_chunk_ids(chunks):\n",
    "  last_page_id = None\n",
    "  current_chunk_index = 0\n",
    "\n",
    "  for chunk in chunks:\n",
    "    source = chunk.metadata.get(\"source\")\n",
    "    page = chunk.metadata.get(\"page\")\n",
    "    current_page_id = f\"{source}:{page}\"\n",
    "\n",
    "    if current_page_id == last_page_id:\n",
    "      current_chunk_index += 1\n",
    "    else:\n",
    "      current_chunk_index = 0\n",
    "\n",
    "    chunk_id = f\"{current_page_id}:{current_chunk_index}\"\n",
    "    last_page_id = current_page_id\n",
    "    chunk.metadata[\"id\"] = chunk_id\n",
    "\n",
    "  print(f\"Calculated chunk IDs for {len(chunks)} chunks.\")\n",
    "  return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.embeddings.ollama import OllamaEmbeddings\n",
    "\n",
    "# embeddings = OllamaEmbeddings(model=\"nomic-embed-text\")\n",
    "embeddings = OllamaEmbeddings(model=\"llama3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add chunks to the Chroma database\n",
    "# It only adds new chunks based on their IDs.\n",
    "\n",
    "db = Chroma(\n",
    "  persist_directory=CHROMA_PATH, \n",
    "  embedding_function=embeddings\n",
    ")\n",
    "\n",
    "chunks_with_ids = calculate_chunk_ids(chunks)\n",
    "existing_items = db.get(include=[])\n",
    "existing_ids = set(existing_items[\"ids\"])\n",
    "\n",
    "print(f\"Number of existing documents in DB: {len(existing_ids)}\")\n",
    "\n",
    "new_chunks = [chunk for chunk in chunks_with_ids if chunk.metadata[\"id\"] not in existing_ids]\n",
    "\n",
    "if new_chunks:\n",
    "  print(f\"ðŸ‘‰ Adding new documents: {len(new_chunks)}\")\n",
    "  new_chunk_ids = [chunk.metadata[\"id\"] for chunk in new_chunks]\n",
    "  db.add_documents(new_chunks, ids=new_chunk_ids)\n",
    "  db.persist()\n",
    "  print(f\"âœ… Successfully added {len(new_chunks)} new documents.\")\n",
    "else:\n",
    "  print(\"âœ… No new documents to add\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query RAG function\n",
    "\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain_community.llms.ollama import Ollama\n",
    "\n",
    "PROMPT_TEMPLATE = \"\"\"\n",
    "Answer the question based only on the following context:\n",
    "\n",
    "{context}\n",
    "\n",
    "---\n",
    "\n",
    "Answer the question based on the above context: {question}\n",
    "\"\"\"\n",
    "\n",
    "# Query Chroma DB using given query text, retrieve relevant chunks, and generate reponse using context\n",
    "\n",
    "def query_rag(query_text: str):\n",
    "  db = Chroma(persist_directory=CHROMA_PATH, embedding_function=embeddings)\n",
    "\n",
    "  results = db.similarity_search_with_score(query_text, k=5)\n",
    "  context_text = \"\\n\\n---\\n\\n\".join([doc.page_content for doc, _score in results])\n",
    "\n",
    "  prompt_template = ChatPromptTemplate.from_template(PROMPT_TEMPLATE)\n",
    "  prompt = prompt_template.format(context=context_text, question=query_text)\n",
    "\n",
    "  model = Ollama(model=\"llama3\")\n",
    "  response_text = model.invoke(prompt)\n",
    "\n",
    "  sources = [doc.metadata.get(\"id\", None) for doc, _score in results]\n",
    "  formatted_response = f\"Response: {response_text}\\nSources: {sources}\"\n",
    "  print(formatted_response)\n",
    "  return response_text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_rag(\"What is this document about?\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
